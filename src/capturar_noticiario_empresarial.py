import requests
from bs4 import BeautifulSoup
import json
import os
from datetime import datetime
import re
from urllib.parse import urlparse, parse_qs

def obter_pasta_ticker(ticker):
    """
    Determina a pasta correta para salvar os dados do ticker.
    """
    base_path = "balancos"
    ticker_base = re.sub(r'\d+$', '', ticker.upper())
    
    if os.path.exists(base_path):
        for pasta in os.listdir(base_path):
            if pasta.startswith(ticker_base):
                return os.path.join(base_path, pasta)
    
    pasta_ticker = os.path.join(base_path, ticker.upper())
    os.makedirs(pasta_ticker, exist_ok=True)
    return pasta_ticker

def buscar_nome_empresa_ticker(ticker):
    """
    Busca o nome da empresa associada ao ticker no mapeamento_b3_consolidado.csv.
    """
    try:
        if os.path.exists('mapeamento_b3_consolidado.csv'):
            import csv
            with open('mapeamento_b3_consolidado.csv', 'r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f, delimiter=';')
                ticker_limpo = ticker.upper().strip()
                
                for row in reader:
                    # Coluna ticker pode ter m√∫ltiplos tickers separados por ;
                    tickers_linha = row['ticker'].split(';')
                    for t in tickers_linha:
                        if t.strip().upper() == ticker_limpo:
                            return row['empresa'].strip()
        
        return ticker
    except Exception as e:
        print(f"Erro ao buscar nome da empresa: {e}")
        return ticker


def extrair_data_publicacao(item):
    """
    Extrai a data de publica√ß√£o do item RSS do Google News.
    """
    try:
        pub_date = item.find('pubDate')
        if pub_date:
            # Formato: Wed, 08 Jan 2026 14:30:00 GMT
            data_str = pub_date.text
            data_obj = datetime.strptime(data_str, '%a, %d %b %Y %H:%M:%S %Z')
            return data_obj.strftime('%Y-%m-%d'), data_obj.strftime('%Y-%m-%d %H:%M:%S')
    except:
        pass
    
    # Se falhar, usa data atual
    agora = datetime.now()
    return agora.strftime('%Y-%m-%d'), agora.strftime('%Y-%m-%d %H:%M:%S')

def extrair_fonte_noticia(link):
    """
    Extrai a fonte da not√≠cia a partir da URL.
    """
    try:
        # Google News usa URLs redirecionadas, tenta extrair dom√≠nio real
        if 'news.google.com' in link:
            # Tenta extrair URL real dos par√¢metros
            parsed = urlparse(link)
            params = parse_qs(parsed.query)
            
            if 'url' in params:
                url_real = params['url'][0]
                dominio = urlparse(url_real).netloc
            else:
                return 'Google News'
        else:
            dominio = urlparse(link).netloc
        
        # Limpa o dom√≠nio
        dominio = dominio.replace('www.', '')
        
        # Mapeia fontes conhecidas
        fontes_conhecidas = {
            'infomoney.com.br': 'InfoMoney',
            'valorinveste.globo.com': 'Valor Investe',
            'valor.globo.com': 'Valor Econ√¥mico',
            'economia.uol.com.br': 'UOL Economia',
            'moneytimes.com.br': 'Money Times',
            'exame.com': 'Exame',
            'estadao.com.br': 'Estad√£o',
            'folha.uol.com.br': 'Folha de S.Paulo',
            'g1.globo.com': 'G1',
            'cnnbrasil.com.br': 'CNN Brasil',
            'seudinheiro.com': 'Seu Dinheiro',
            'investnews.com.br': 'InvestNews'
        }
        
        for dominio_chave, nome_fonte in fontes_conhecidas.items():
            if dominio_chave in dominio:
                return nome_fonte
        
        # Se n√£o encontrar, retorna dom√≠nio capitalizado
        return dominio.split('.')[0].capitalize()
    
    except:
        return 'Desconhecida'

def limpar_descricao_html(descricao):
    """
    Remove tags HTML da descri√ß√£o e limpa o texto.
    """
    try:
        # Remove tags HTML
        soup = BeautifulSoup(descricao, 'html.parser')
        texto = soup.get_text()
        
        # Remove espa√ßos extras
        texto = ' '.join(texto.split())
        
        # Limita a 300 caracteres
        if len(texto) > 300:
            texto = texto[:297] + '...'
        
        return texto
    except:
        return descricao

def buscar_noticiario_empresarial(ticker):
    """
    Busca not√≠cias do mercado sobre a empresa via Google News RSS.
    """
    try:
        ticker_clean = re.sub(r'\d+$', '', ticker.upper())
        nome_empresa = buscar_nome_empresa_ticker(ticker)
        
        # Monta query de busca mais espec√≠fica
        query = f'{nome_empresa} OR {ticker_clean} bolsa a√ß√µes'
        
        # URL do Google News RSS
        url = f'https://news.google.com/rss/search?q={query}&hl=pt-BR&gl=BR&ceid=BR:pt-419'
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=15)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'xml')
            itens = soup.find_all('item')
            
            noticias = []
            
            for item in itens[:30]:  # Limita a 30 not√≠cias mais recentes
                try:
                    titulo = item.find('title').text if item.find('title') else "T√≠tulo n√£o dispon√≠vel"
                    link = item.find('link').text if item.find('link') else None
                    descricao = item.find('description').text if item.find('description') else "Descri√ß√£o n√£o dispon√≠vel"
                    
                    # Limpa descri√ß√£o HTML
                    descricao_limpa = limpar_descricao_html(descricao)
                    
                    # Extrai data de publica√ß√£o
                    data, data_hora = extrair_data_publicacao(item)
                    
                    # Extrai fonte
                    fonte = extrair_fonte_noticia(link) if link else 'Desconhecida'
                    
                    # Gera ID √∫nico baseado no t√≠tulo e data
                    id_noticia = hash(f"{titulo}{data_hora}") & 0x7FFFFFFF
                    
                    noticia = {
                        'id': id_noticia,
                        'data': data,
                        'data_hora': data_hora,
                        'titulo': titulo.strip(),
                        'descricao': descricao_limpa,
                        'fonte': fonte,
                        'url': link,
                        'tipo': 'noticia_mercado'
                    }
                    
                    noticias.append(noticia)
                
                except Exception as e:
                    print(f"Erro ao processar item de not√≠cia: {e}")
                    continue
            
            return noticias
        else:
            print(f"Erro ao buscar not√≠cias: Status {response.status_code}")
            return []
    
    except Exception as e:
        print(f"Erro ao buscar notici√°rio empresarial: {e}")
        return []

def salvar_noticiario_json(ticker, noticias):
    """
    Salva o notici√°rio em formato JSON na pasta do ticker.
    Acumula com not√≠cias existentes, evitando duplicatas por ID.
    """
    try:
        pasta_ticker = obter_pasta_ticker(ticker)
        arquivo_json = os.path.join(pasta_ticker, 'noticiario.json')
        
        # Carrega not√≠cias existentes se o arquivo j√° existir
        noticias_existentes = []
        if os.path.exists(arquivo_json):
            with open(arquivo_json, 'r', encoding='utf-8') as f:
                dados_existentes = json.load(f)
                noticias_existentes = dados_existentes.get('noticias', [])
        
        # Cria dicion√°rio de not√≠cias existentes por ID
        noticias_dict = {}
        for noticia in noticias_existentes:
            if noticia.get('id'):
                noticias_dict[noticia['id']] = noticia
        
        # Adiciona novas not√≠cias (substitui se ID j√° existir)
        for noticia in noticias:
            if noticia.get('id'):
                noticias_dict[noticia['id']] = noticia
        
        # Converte de volta para lista e ordena por data
        noticias_finais = list(noticias_dict.values())
        noticias_finais.sort(key=lambda x: x['data_hora'], reverse=True)
        
        # Limita a 100 not√≠cias mais recentes para n√£o crescer indefinidamente
        noticias_finais = noticias_finais[:100]
        
        # Busca informa√ß√µes da empresa
        nome_empresa = buscar_nome_empresa_ticker(ticker)
        ticker_limpo = re.sub(r'\d+$', '', ticker.upper())
        
        # Monta estrutura final
        dados_finais = {
            'empresa': {
                'ticker': ticker_limpo,
                'nome': nome_empresa
            },
            'ultima_atualizacao': datetime.now().isoformat(),
            'total_noticias': len(noticias_finais),
            'fonte': 'Google News',
            'noticias': noticias_finais
        }
        
        # Salva no arquivo JSON
        with open(arquivo_json, 'w', encoding='utf-8') as f:
            json.dump(dados_finais, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ {len(noticias_finais)} not√≠cias salvas em: {arquivo_json}")
        return arquivo_json
    
    except Exception as e:
        print(f"‚ùå Erro ao salvar notici√°rio: {e}")
        return None

def exibir_noticiario_formatado(noticias, limite=5):
    """
    Exibe o notici√°rio em formato bonito e amig√°vel.
    """
    if not noticias:
        print("Nenhuma not√≠cia encontrada.")
        return
    
    noticias_exibir = noticias[:limite]
    
    print(f"\n{'=' * 100}")
    print(f"üì∞ NOTICI√ÅRIO EMPRESARIAL ({len(noticias)} not√≠cias)")
    print(f"{'=' * 100}\n")
    
    for i, noticia in enumerate(noticias_exibir, 1):
        print(f"{'-' * 100}")
        print(f"{i}. \033[1m{noticia['titulo']}\033[0m")
        print(f"   üìÖ {noticia['data_hora']} | üì∞ {noticia['fonte']}")
        print(f"   \033[94m{noticia['descricao']}\033[0m")
        if noticia.get('url'):
            print(f"   üîó {noticia['url']}")
        print(f"{'-' * 100}\n")

def main():
    """
    Fun√ß√£o principal para executar a captura de notici√°rio.
    """
    import sys
    
    if len(sys.argv) < 2:
        print("‚ùå Uso: python capturar_noticiario_empresarial.py TICKER")
        print("   Exemplo: python capturar_noticiario_empresarial.py ABEV3")
        sys.exit(1)
    
    ticker = sys.argv[1].upper()
    
    print(f"üîç Buscando notici√°rio empresarial de {ticker}...")
    print(f"‚è≥ Aguarde, isso pode levar alguns segundos...\n")
    
    noticias = buscar_noticiario_empresarial(ticker)
    
    if noticias:
        print(f"‚úÖ {len(noticias)} not√≠cias encontradas!")
        
        # Salva em JSON
        arquivo_salvo = salvar_noticiario_json(ticker, noticias)
        
        if arquivo_salvo:
            # Exibe preview
            exibir_noticiario_formatado(noticias, limite=5)
            print(f"\nüíæ Arquivo salvo: {arquivo_salvo}")
        else:
            print("\n‚ùå Erro ao salvar arquivo JSON")
    else:
        print("‚ùå Nenhuma not√≠cia encontrada ou erro na busca")

if __name__ == "__main__":
    main()
