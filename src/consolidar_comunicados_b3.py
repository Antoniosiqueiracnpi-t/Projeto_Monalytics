"""
CONSOLIDADOR DE COMUNICADOS B3 - Feed √önico para Web

Gera arquivo JSON consolidado com todas as not√≠cias de todas as empresas.
Ideal para consumo em p√°ginas HTML/frontend.

SA√çDA:
- Arquivo: balancos/feed_noticias.json
- Formato: JSON estruturado com todas as not√≠cias
- Ordena√ß√£o: Data decrescente (mais recente primeiro)
- Atualiza√ß√£o: Autom√°tica via GitHub Actions (di√°ria)

ESTRUTURA DO JSON:
{
  "meta": {
    "ultima_atualizacao": "2024-12-30T09:00:00Z",
    "total_empresas": 50,
    "total_noticias": 750,
    "periodo_dias": 30
  },
  "estatisticas": {
    "por_categoria": {...},
    "por_ticker": {...}
  },
  "feed": [
    {
      "data": "2024-12-30",
      "hora": "09:00:00",
      "empresa": {...},
      "noticia": {...}
    }
  ]
}
"""

import json
from pathlib import Path
from datetime import datetime
from collections import Counter
from typing import List, Dict


class ConsolidadorNoticias:
    """
    Consolida todas as not√≠cias individuais em um √∫nico JSON.
    """
    
    def __init__(self, pasta_balancos: str = "balancos"):
        self.pasta_balancos = Path(pasta_balancos)
        self.empresas_processadas = 0
        self.total_noticias = 0
    
    def consolidar(self) -> Dict:
        """
        Consolida todas as not√≠cias em uma √∫nica estrutura.
        """
        print("="*70)
        print("üìä CONSOLIDANDO NOT√çCIAS PARA WEB")
        print("="*70)
        
        # Buscar todos os arquivos noticias.json
        arquivos_noticias = list(self.pasta_balancos.glob("*/noticias.json"))
        
        if not arquivos_noticias:
            print("‚ö†Ô∏è  Nenhum arquivo de not√≠cias encontrado")
            return self._estrutura_vazia()
        
        print(f"\nüìÅ Arquivos encontrados: {len(arquivos_noticias)}")
        
        # Coletar todas as not√≠cias
        feed = []
        categorias = Counter()
        tickers = Counter()
        periodo_min = None
        periodo_max = None
        
        for arquivo in arquivos_noticias:
            try:
                with open(arquivo, 'r', encoding='utf-8') as f:
                    dados = json.load(f)
                
                empresa_info = dados['empresa']
                ticker = empresa_info['ticker']
              
                # Normaliza ticker para sempre ter classe quando o JSON vier sem (ex.: 'BBAS' -> 'BBAS3')
                try:
                    t_norm = str(ticker).strip().upper()
                    if t_norm and not re.search(r'\d{1,2}$', t_norm):
                        base = t_norm[:4]
                        if base in self._mapa_base_para_ticker_classe:
                            ticker = self._mapa_base_para_ticker_classe[base]
                            empresa_info['ticker'] = ticker
                except Exception:
                    pass
              
                
                self.empresas_processadas += 1
                
                # Processar cada not√≠cia
                for noticia in dados['noticias']:
                    # Adicionar informa√ß√£o da empresa √† not√≠cia
                    item_feed = {
                        'data': noticia['data'],
                        'hora': self._extrair_hora(noticia.get('titulo', '')),
                        'empresa': {
                            'ticker': ticker,
                            'nome': empresa_info['nome'],
                            'cnpj': empresa_info.get('cnpj', '')
                        },
                        'noticia': {
                            'titulo': noticia['titulo'],
                            'headline': noticia['headline'],
                            'categoria': noticia['categoria'],
                            'url': noticia['url']
                        }
                    }
                    
                    feed.append(item_feed)
                    categorias[noticia['categoria']] += 1
                    tickers[ticker] += 1
                    self.total_noticias += 1
                    
                    # Atualizar range de datas
                    data = noticia['data']
                    if periodo_min is None or data < periodo_min:
                        periodo_min = data
                    if periodo_max is None or data > periodo_max:
                        periodo_max = data
                
                print(f"  ‚úÖ {ticker}: {len(dados['noticias'])} not√≠cias")
                
            except Exception as e:
                print(f"  ‚ùå Erro ao processar {arquivo}: {e}")
                continue
        
        # Ordenar feed por data (mais recente primeiro)
        feed.sort(key=lambda x: (x['data'], x['hora']), reverse=True)
        
        # Criar estrutura consolidada
        consolidado = {
            'meta': {
                'ultima_atualizacao': datetime.now().isoformat() + 'Z',
                'data_atualizacao': datetime.now().strftime('%Y-%m-%d'),
                'hora_atualizacao': datetime.now().strftime('%H:%M:%S'),
                'total_empresas': self.empresas_processadas,
                'total_noticias': self.total_noticias,
                'periodo': {
                    'inicio': periodo_min,
                    'fim': periodo_max
                }
            },
            'estatisticas': {
                'por_categoria': dict(categorias.most_common()),
                'por_ticker': dict(tickers.most_common(20)),  # Top 20
                'top_categorias': [
                    {'categoria': cat, 'total': count}
                    for cat, count in categorias.most_common(10)
                ],
                'top_empresas': [
                    {'ticker': ticker, 'total': count}
                    for ticker, count in tickers.most_common(10)
                ]
            },
            'feed': feed
        }
        
        # Estat√≠sticas
        print(f"\n{'='*70}")
        print(f"üìä ESTAT√çSTICAS:")
        print(f"{'='*70}")
        print(f"‚úÖ Empresas processadas: {self.empresas_processadas}")
        print(f"‚úÖ Total de not√≠cias: {self.total_noticias}")
        print(f"‚úÖ Per√≠odo: {periodo_min} a {periodo_max}")
        
        print(f"\nüìà TOP 5 CATEGORIAS:")
        for i, (cat, count) in enumerate(categorias.most_common(5), 1):
            print(f"  {i}. {cat}: {count} not√≠cias")
        
        print(f"\nüìà TOP 5 EMPRESAS:")
        for i, (ticker, count) in enumerate(tickers.most_common(5), 1):
            print(f"  {i}. {ticker}: {count} not√≠cias")
        
        return consolidado
    
    def _extrair_hora(self, titulo: str) -> str:
        """
        Extrai hora do t√≠tulo se dispon√≠vel.
        Retorna '00:00:00' como padr√£o.
        """
        import re
        # Tentar extrair hora do formato DD/MM/AAAA HH:MM
        match = re.search(r'(\d{2}):(\d{2})', titulo)
        if match:
            return f"{match.group(1)}:{match.group(2)}:00"
        return "00:00:00"
    
    def _estrutura_vazia(self) -> Dict:
        """Retorna estrutura vazia quando n√£o h√° not√≠cias."""
        return {
            'meta': {
                'ultima_atualizacao': datetime.now().isoformat() + 'Z',
                'data_atualizacao': datetime.now().strftime('%Y-%m-%d'),
                'hora_atualizacao': datetime.now().strftime('%H:%M:%S'),
                'total_empresas': 0,
                'total_noticias': 0,
                'periodo': {
                    'inicio': None,
                    'fim': None
                }
            },
            'estatisticas': {
                'por_categoria': {},
                'por_ticker': {},
                'top_categorias': [],
                'top_empresas': []
            },
            'feed': []
        }
    
    def salvar(self, dados: Dict, arquivo: str = "feed_noticias.json"):
        """
        Salva JSON consolidado.
        """
        output_path = self.pasta_balancos / arquivo
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(dados, f, ensure_ascii=False, indent=2)
        
        # Calcular tamanho
        tamanho_kb = output_path.stat().st_size / 1024
        
        print(f"\n{'='*70}")
        print(f"‚úÖ ARQUIVO GERADO COM SUCESSO!")
        print(f"{'='*70}")
        print(f"üìÅ Local: {output_path}")
        print(f"üìä Tamanho: {tamanho_kb:.2f} KB")
        print(f"üåê URL: balancos/{arquivo}")
        print(f"{'='*70}\n")
        
        return output_path


# ============================================================================
# MAIN
# ============================================================================

def main():
    consolidador = ConsolidadorNoticias()
    
    # Consolidar todas as not√≠cias
    dados = consolidador.consolidar()
    
    # Salvar JSON √∫nico
    consolidador.salvar(dados)
    
    print("‚úÖ Consolida√ß√£o conclu√≠da!")
    print("üí° Arquivo pronto para consumo em p√°ginas HTML")
    print("üí° Atualiza√ß√£o di√°ria autom√°tica via GitHub Actions\n")


if __name__ == "__main__":
    main()
